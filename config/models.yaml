# Model Configuration for Multi-Agent System with llama.cpp
# 
# This file configures model paths and parameters for all agents.
# Update the model_path values to point to your actual GGUF model files.

models:
  # Triage Agent - Classifies and analyzes incoming requests
  triage:
    model_path: "path_to/model"  # Path to your triage model
    n_gpu_layers: 30                        # Number of layers to offload to GPU
    n_ctx: 2048                            # Context window size
    verbose: false                         # Enable verbose model loading output
    
  # Security Agent - Risk assessment and whitelist management  
  # TODO: Will be used when security agent is converted to llama.cpp
  security:
    model_path: "path_to/model"
    n_gpu_layers: 30
    n_ctx: 2048
    verbose: false
    
  # Executor Agent - Command execution with conversation management
  # TODO: Will be used when executor agent is converted to llama.cpp  
  executor:
    model_path: "path_to/model"
    n_gpu_layers: 30
    n_ctx: 2048
    verbose: false

# Global settings affecting all models
global_settings:
  # Automatically detect optimal GPU layer count based on available VRAM
  auto_detect_gpu_layers: true
  
  # Fall back to CPU-only mode if GPU initialization fails
  fallback_to_cpu: true
  
  # Maximum time (seconds) to wait for model operations
  model_timeout: 300
  
  # Interval (seconds) for automatic cleanup of unused resources
  cleanup_interval: 3600

# Model recommendations by size and use case:
#
# Small models (1-3B parameters, ~2-4GB VRAM):
#   - Good for: Basic classification, simple reasoning
#   - Examples: Llama 3.2 1B/3B, Phi-3.5 Mini, Qwen2.5 1.5B/3B
#   - GPU layers: 20-35 (adjust based on VRAM)
#   - Context: 2048-4096
#
# Medium models (7-8B parameters, ~6-8GB VRAM):
#   - Good for: Complex reasoning, detailed analysis
#   - Examples: Llama 3.1 8B, Mistral 7B, Qwen2.5 7B
#   - GPU layers: 35-45 (adjust based on VRAM)  
#   - Context: 4096-8192
#
# Large models (13-70B parameters, 10+ GB VRAM):
#   - Good for: Advanced reasoning, multi-step tasks
#   - Examples: Llama 3.1 70B, Qwen2.5 14B/32B
#   - GPU layers: Varies greatly by model size and VRAM
#   - Context: 8192+
#
# Tips:
#   - Start with lower n_gpu_layers and increase gradually
#   - Monitor VRAM usage and adjust accordingly
#   - Use same model for all agents initially, then specialize
#   - Quantized models (Q4_K_M, Q5_K_M) offer good quality/size balance
